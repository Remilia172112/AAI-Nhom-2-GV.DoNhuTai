{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKcR43PVf99G"
      },
      "source": [
        "# Adversarial Search: Playing Dots and Boxes\n",
        "\n",
        "\n",
        "## Instructions\n",
        "\n",
        "Total Points: Undegraduates 100, graduate students 110\n",
        "\n",
        "Complete this notebook and submit it. The notebook needs to be a complete project report with your implementation, documentation including a short discussion of how your implementation works and your design choices, and experimental results (e.g., tables and charts with simulation results) with a short discussion of what they mean. Use the provided notebook cells and insert additional code and markdown cells as needed.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "You will implement different versions of agents that play the game Dots and Boxes:\n",
        "\n",
        "> \"Dots and Boxes is a pencil-and-paper game for two players. The game starts with an empty grid of dots. Usually two players take turns adding a single horizontal or vertical line between two unjoined adjacent dots. A player who completes the fourth side of a 1x1 box earns one point and takes another turn. A point is typically recorded by placing a mark that identifies the player in the box, such as an initial. The game ends when no more lines can be placed. The winner is the player with the most points. The board may be of any size grid.\" (see [Dots and Boxes on Wikipedia](https://en.wikipedia.org/wiki/Dots_and_Boxes))\n",
        "\n",
        "You can play Dots and Boxes [here](https://www.math.ucla.edu/~tom/Games/dots&boxes.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB78PuN2f99K"
      },
      "source": [
        "## Task 1: Defining the Search Problem [10 point]\n",
        "\n",
        "Define the components of the search problem associated with this game:\n",
        "\n",
        "* Initial state\n",
        "* Actions\n",
        "* Transition model\n",
        "* Test for the terminal state\n",
        "* Utility for terminal states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pqdfne54f99K"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def initial_state(n=3, m=3):\n",
        "    \"\"\"\n",
        "    Mỗi ô: [[edges], edge_count, owner]\n",
        "    edges: [top, bottom, left, right], 0 = chưa đánh, -1 = đã đánh\n",
        "    owner: 1 (Player 1), -1 (Player 2), 0 (chưa ai)\n",
        "    \"\"\"\n",
        "    return [[[ [0, 0, 0, 0], 0, 0 ] for _ in range(m-1)] for _ in range(n-1)]\n",
        "\n",
        "\n",
        "def actions(state, posx, posy, tag, player):\n",
        "    \"\"\"\n",
        "    posx,posy: chỉ số ô (0..R-1, 0..C-1)\n",
        "    tag: 0=top, 1=bottom, 2=left, 3=right\n",
        "    player: 1 hoặc -1 (chúng ta lưu cạnh = -1 khi vẽ)\n",
        "    \"\"\"\n",
        "    a, b = len(state), len(state[0])\n",
        "    # nếu cạnh đã được vẽ (khác 0) thì bỏ qua\n",
        "    if state[posx][posy][0][tag] != 0:\n",
        "        return\n",
        "\n",
        "    # đánh dấu cạnh = -1 (chỉ dùng -1 để biểu diễn \"đã vẽ\")\n",
        "    state[posx][posy][0][tag] = -1\n",
        "\n",
        "    # cập nhật ô liền kề (nếu có) và tính lại edge_count cho cả hai ô liên quan\n",
        "    if tag == 0 and posx > 0:              # top -> ảnh hưởng bottom của ô trên\n",
        "        state[posx-1][posy][0][1] = -1\n",
        "    elif tag == 1 and posx < a-1:          # bottom -> ảnh hưởng top của ô dưới\n",
        "        state[posx+1][posy][0][0] = -1\n",
        "    elif tag == 2 and posy > 0:            # left -> ảnh hưởng right của ô trái\n",
        "        state[posx][posy-1][0][3] = -1\n",
        "    elif tag == 3 and posy < b-1:          # right -> ảnh hưởng left của ô phải\n",
        "        state[posx][posy+1][0][2] = -1\n",
        "\n",
        "    # cập nhật count cho ô hiện tại\n",
        "    state[posx][posy][1] = sum(1 for e in state[posx][posy][0] if e != 0)\n",
        "\n",
        "    # cập nhật count cho ô liền kề (nếu có)\n",
        "    if tag == 0 and posx > 0:\n",
        "        state[posx-1][posy][1] = sum(1 for e in state[posx-1][posy][0] if e != 0)\n",
        "    elif tag == 1 and posx < a-1:\n",
        "        state[posx+1][posy][1] = sum(1 for e in state[posx+1][posy][0] if e != 0)\n",
        "    elif tag == 2 and posy > 0:\n",
        "        state[posx][posy-1][1] = sum(1 for e in state[posx][posy-1][0] if e != 0)\n",
        "    elif tag == 3 and posy < b-1:\n",
        "        state[posx][posy+1][1] = sum(1 for e in state[posx][posy+1][0] if e != 0)\n",
        "\n",
        "\n",
        "def terminal_state(state):\n",
        "    \"\"\"Trả về 0 nếu chưa kết thúc, 1/-1 nếu thắng, 2 nếu hòa.\"\"\"\n",
        "    a, b = len(state), len(state[0])\n",
        "    p1, p2, total = 0, 0, a*b\n",
        "    for i in range(a):\n",
        "        for j in range(b):\n",
        "            if state[i][j][2] == 1:\n",
        "                p1 += 1\n",
        "            elif state[i][j][2] == -1:\n",
        "                p2 += 1\n",
        "\n",
        "    # kiểm tra còn cạnh trống không\n",
        "    any_open = any(0 in state[i][j][0] for i in range(a) for j in range(b))\n",
        "    if any_open:\n",
        "        return 0\n",
        "\n",
        "    # không còn cạnh → kết thúc\n",
        "    if p1 > p2:\n",
        "        return 1\n",
        "    elif p2 > p1:\n",
        "        return -1\n",
        "    else:\n",
        "        return 2\n",
        "\n",
        "\n",
        "def utility(state):\n",
        "    res = terminal_state(state)\n",
        "    if res == 1:\n",
        "        return \"Win\"\n",
        "    elif res == -1:\n",
        "        return \"Lose\"\n",
        "    elif res == 2:\n",
        "        return \"Draw\"\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "def transition_model(action, search, utility, state, player):\n",
        "    flag = -1*player\n",
        "    \"\"\"Thực hiện 1 lượt chơi.\"\"\"\n",
        "    result = utility(state)\n",
        "    if result:\n",
        "        return result\n",
        "\n",
        "    choice = search(state)\n",
        "    if not choice:\n",
        "        return state  # không còn nước đi\n",
        "    i, j, k = choice\n",
        "    action(state, i, j, k, player)\n",
        "\n",
        "    # cập nhật ô nào hoàn tất\n",
        "    for x in range(len(state)):\n",
        "        for y in range(len(state[0])):\n",
        "            if state[x][y][1] == 4 and state[x][y][2] == 0:\n",
        "                state[x][y][2] = player\n",
        "                flag = player\n",
        "\n",
        "    return state, flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lW1ftaSKf99M"
      },
      "source": [
        "How big is the state space? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w95fdf3_f99M"
      },
      "outputs": [],
      "source": [
        "# Suppose that n x m has (n-1)x(m-1) squares\n",
        "# First square has 4 sides = 4 state, folling to row and col edge, each square obtain 3 stage and 2 stage in each square inside the point grid\n",
        "# => state space S = 4 + (n - 1)x3 + (m-1)x3 + ((n-2)*2*(m-2)*2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AeIAIP1Tf99N"
      },
      "source": [
        "How big is the game tree that minimax search will go through? Give an estimate and explain it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f5qsTusf99N"
      },
      "outputs": [],
      "source": [
        "# S_minimax = S(S-1)(S-2)...(S - k), k is the state when half of total square belong to a specific player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY5DFssDf99N"
      },
      "source": [
        "## Task 2: Game Environment and Random Agent [30 point]\n",
        "\n",
        "You need to think about a data structure to represent the board meaning he placed lines and who finished what box. There are many options. Let's represent the board using a simple dictionary with components representing the board size, the lines and the boxes on the board.\n",
        "\n",
        "**Important:** Everybody needs to use the same representation so we can let agents play against each other later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct0pUEFYf99O",
        "outputId": "135b85c5-3a7d-42fc-dc7e-26ecb137397c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'size': (4, 4),\n",
              " 'lines': {('h', 1, 1): True, ('v', 1, 1): True},\n",
              " 'boxes': dict}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "board = {\n",
        "    'size': (4, 4),  ### number of rows and columns of dots\n",
        "    'lines': dict(), ### keys are the set of drawn lines\n",
        "    'boxes': dict    ### keys are the boxes and the value is the player who completed each box\n",
        "}\n",
        "\n",
        "def draw_line(board, orientation, row, col):\n",
        "    \"\"\"\n",
        "    Place a line on an exiting board.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    board: dict\n",
        "        the board\n",
        "    orientation: str\n",
        "        either 'h' or 'v' for horizontal or vertical\n",
        "    row, col: int\n",
        "        index of the starting dot for the line (starting with 0)\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if orientation not in ['h', 'v']:\n",
        "        return False\n",
        "\n",
        "    if row < 0 or col < 0:\n",
        "        return False\n",
        "\n",
        "    if row >= board['size'][0] + (orientation == 'v') or col >= board['size'][1] + (orientation == 'h'):\n",
        "        return False\n",
        "\n",
        "    if (orientation, row, col) in board['lines']:\n",
        "        return False\n",
        "\n",
        "    board[\"lines\"][(orientation, row, col)] = True\n",
        "    return True\n",
        "\n",
        "\n",
        "print(draw_line(board, \"h\", 1, 1))\n",
        "print(draw_line(board, \"v\", 1, 1))\n",
        "\n",
        "# this should not work\n",
        "print(draw_line(board, \"h\", 1, 1))\n",
        "\n",
        "board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRIk-3x6f99P"
      },
      "source": [
        "Write code to display the board. **Bonus point: Post your visualization code with an example output to the discussion board. The best visualization will earn you bonus participation points in this class.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "NBOKSQC7f99P"
      },
      "outputs": [],
      "source": [
        "def get_board(state):\n",
        "    \"\"\"\n",
        "    In board dạng ASCII:\n",
        "    - state: R x C, mỗi ô = [edges, edge_count, owner]\n",
        "    - edges: [top, bottom, left, right] (0 not drawn, -1 drawn)\n",
        "    \"\"\"\n",
        "    R = len(state)        # number of box rows\n",
        "    C = len(state[0])     # number of box cols\n",
        "\n",
        "    # In từng hàng dot (total dot rows = R+1)\n",
        "    for dr in range(R + 1):\n",
        "        # 1) in: dot and horizontal edges for this dot-row\n",
        "        line = \"\"\n",
        "        for dc in range(C):\n",
        "            line += \"o\"\n",
        "            # horizontal edge between dot (dr,dc) and (dr, dc+1)\n",
        "            drawn = False\n",
        "            # if box above exists, its bottom corresponds to this dot-row\n",
        "            if dr - 1 >= 0:\n",
        "                if state[dr-1][dc][0][1] != 0:\n",
        "                    drawn = True\n",
        "            # if box below exists, its top corresponds to this dot-row\n",
        "            if dr < R:\n",
        "                if state[dr][dc][0][0] != 0:\n",
        "                    drawn = True\n",
        "\n",
        "            if drawn:\n",
        "                line += \"──\"   # horizontal drawn\n",
        "            else:\n",
        "                line += \"  \"   # empty space\n",
        "        line += \"o\"\n",
        "        print(line)\n",
        "\n",
        "        # 2) in vertical edges + (optionally) owner placeholders for box row dr (only if dr < R)\n",
        "        if dr < R:\n",
        "            line2 = \"\"\n",
        "            for dc in range(C + 1):\n",
        "                # vertical edge between dot (dr,dc) and (dr+1,dc)\n",
        "                drawn = False\n",
        "                # box to left exists -> its right edge corresponds\n",
        "                if dc - 1 >= 0:\n",
        "                    if state[dr][dc-1][0][3] != 0:\n",
        "                        drawn = True\n",
        "                # box to right exists -> its left edge corresponds\n",
        "                if dc < C:\n",
        "                    if state[dr][dc][0][2] != 0:\n",
        "                        drawn = True\n",
        "\n",
        "                if drawn:\n",
        "                    line2 += \"|\"\n",
        "                else:\n",
        "                    line2 += \" \"\n",
        "\n",
        "                # after vertical char, add two spaces except at final column (we will print small holder)\n",
        "                if dc < C:\n",
        "                    # you can show owner or empty inside box space\n",
        "                    owner = state[dr][dc][2]\n",
        "                    if owner == 1:\n",
        "                        line2 += \"1 \"\n",
        "                    elif owner == -1:\n",
        "                        line2 += \"2 \"\n",
        "                    else:\n",
        "                        line2 += \"  \"\n",
        "            print(line2)\n",
        "    print()  # blank line after board"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F9mTr4-f99Q"
      },
      "source": [
        "Implement helper functions for:\n",
        "\n",
        "* The transition model $result(s, a)$.\n",
        "* The utility function $utility(s)$.\n",
        "* Check for terminal states $terminal(s)$.\n",
        "* A check for available actions in each state $actions(s)$.\n",
        "\n",
        "__Notes:__\n",
        "* Make sure that all these functions work with boards of different sizes (number of columns and rows as stored in the board).\n",
        "* The result function updates the board and evaluates if the player closed a box and needs to store that information on the board. Add elements of the form `(row,col): player` to the board dictionary. `row` and `col` are the coordinates for the box and `player` is +1 or -1 representing the player. For example `(0,0): -1` means that the top-left box belongs to the other player.\n",
        "* _Important:_ Remember that a player goes again after she completes a box!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsC1hnsOf99Q"
      },
      "outputs": [],
      "source": [
        "def Environment(player1, player2, search, times=100, n=3, m=3):\n",
        "    board = initial_state(n, m)\n",
        "    turn = 0\n",
        "    flag = 1\n",
        "    result = None\n",
        "    print(\"Init\")\n",
        "    get_board(board)\n",
        "\n",
        "    while result is None and turn < times:\n",
        "        if flag == 1:\n",
        "            print(f\"\\n--- Turn {turn+1} (Player 1) ---\")\n",
        "            board, f = transition_model(actions, search, utility, board, 1)\n",
        "            flag = f\n",
        "        else:\n",
        "            print(f\"\\n--- Turn {turn+1} (Player 2) ---\")\n",
        "            board, f = transition_model(actions, search, utility, board, -1)\n",
        "            flag = f\n",
        "\n",
        "        get_board(board)\n",
        "        result = utility(board)\n",
        "        turn += 1\n",
        "\n",
        "        # nếu không còn nước đi\n",
        "        if not any(0 in cell[0] for row in board for cell in row):\n",
        "            result = utility(board)\n",
        "            break\n",
        "\n",
        "    print(\"\\n=== Game Over ===\")\n",
        "    print(\"Result:\", result)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLeWfxk9f99Q"
      },
      "source": [
        "Implement an agent that plays randomly. Make sure the agent function receives as the percept the board and returns a valid action. Use an agent function definition with the following signature (arguments):\n",
        "\n",
        "`def random_player(board, player = None): ...`\n",
        "\n",
        "The argument `player` is used for agents that do not store what side they are playing. The value passed on by the environment should be 1 ot -1 for playerred and yellow, respectively.  See [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) for an example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1vuunr32f99Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def random_player(state, player=None):\n",
        "    a, b = len(state), len(state[0])\n",
        "    available = []\n",
        "    for i in range(a):\n",
        "        for j in range(b):\n",
        "            for k in range(4):\n",
        "                if state[i][j][0][k] == 0:\n",
        "                    available.append((i, j, k))\n",
        "    return random.choice(available) if available else None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nq3rmfOKf99Q"
      },
      "source": [
        "Let two random agents play against each other 1000 times. Look at the [Experiments section for tic-tac-toe](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_and_or_tree_search.ipynb#Experiments) to see how the environment uses the agent functions to play against each other.\n",
        "\n",
        "How often does each player win? Is the result expected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "__CUYpxdf99Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f6487170-84df-4e09-885f-d9947f75558f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Init\n",
            "o  o  o\n",
            "       \n",
            "o  o  o\n",
            "       \n",
            "o  o  o\n",
            "\n",
            "\n",
            "--- Turn 1 (Player 1) ---\n",
            "o  o  o\n",
            "       \n",
            "o  o  o\n",
            "   |   \n",
            "o  o  o\n",
            "\n",
            "\n",
            "--- Turn 2 (Player 2) ---\n",
            "o  o  o\n",
            "       \n",
            "o  o  o\n",
            "   |   \n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 3 (Player 1) ---\n",
            "o  o  o\n",
            "|      \n",
            "o  o  o\n",
            "   |   \n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 4 (Player 2) ---\n",
            "o  o  o\n",
            "|      \n",
            "o  o  o\n",
            "   |  |\n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 5 (Player 1) ---\n",
            "o  o  o\n",
            "|  |   \n",
            "o  o  o\n",
            "   |  |\n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 6 (Player 2) ---\n",
            "o  o  o\n",
            "|  |   \n",
            "o──o  o\n",
            "   |  |\n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 7 (Player 1) ---\n",
            "o  o  o\n",
            "|  |   \n",
            "o──o──o\n",
            "   |1 |\n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 8 (Player 1) ---\n",
            "o  o──o\n",
            "|  |   \n",
            "o──o──o\n",
            "   |1 |\n",
            "o  o──o\n",
            "\n",
            "\n",
            "--- Turn 9 (Player 2) ---\n",
            "o  o──o\n",
            "|  |   \n",
            "o──o──o\n",
            "   |1 |\n",
            "o──o──o\n",
            "\n",
            "\n",
            "--- Turn 10 (Player 1) ---\n",
            "o──o──o\n",
            "|1 |   \n",
            "o──o──o\n",
            "   |1 |\n",
            "o──o──o\n",
            "\n",
            "\n",
            "--- Turn 11 (Player 1) ---\n",
            "o──o──o\n",
            "|1 |   \n",
            "o──o──o\n",
            "|1 |1 |\n",
            "o──o──o\n",
            "\n",
            "\n",
            "--- Turn 12 (Player 1) ---\n",
            "o──o──o\n",
            "|1 |1 |\n",
            "o──o──o\n",
            "|1 |1 |\n",
            "o──o──o\n",
            "\n",
            "\n",
            "=== Game Over ===\n",
            "Result: Win\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Win'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "Environment(None, None, random_player, n=3, m=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0inLjqMef99R"
      },
      "source": [
        "## Task 3: Minimax Search with Alpha-Beta Pruning [30 points]\n",
        "\n",
        "### Implement the search starting.\n",
        "\n",
        "Implement the search starting from a given board and specifying the player and put it into an agent function.\n",
        "You can use code from the [tic-tac-toe example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_alpha_beta_tree_search.ipynb).\n",
        "\n",
        "__Notes:__\n",
        "* Make sure that all your agent functions have a signature consistent with the random agent above.\n",
        "* The search space for larger board may be too large. You can experiment with smaller boards.\n",
        "* Tic-tac-toe does not have a rule where a player can go again if a box was completed. You need to adapt the tree search to reflect that rule."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLWCJpJ9f99R"
      },
      "outputs": [],
      "source": [
        "import copy, math, random\n",
        "\n",
        "def legal_moves(state):\n",
        "    moves = []\n",
        "    for i in range(len(state)):\n",
        "        for j in range(len(state[0])):\n",
        "            for k in range(4):\n",
        "                if state[i][j][0][k] == 0:\n",
        "                    moves.append((i, j, k))\n",
        "    return moves\n",
        "\n",
        "\n",
        "def apply_action(state, i, j, k, player):\n",
        "    a, b = len(state), len(state[0])\n",
        "    if state[i][j][0][k] != 0:\n",
        "        return 0  # cạnh đã đánh rồi\n",
        "\n",
        "    state[i][j][0][k] = -1\n",
        "    # cập nhật cạnh liền kề\n",
        "    if k == 0 and i > 0: state[i-1][j][0][1] = -1\n",
        "    elif k == 1 and i < a-1: state[i+1][j][0][0] = -1\n",
        "    elif k == 2 and j > 0: state[i][j-1][0][3] = -1\n",
        "    elif k == 3 and j < b-1: state[i][j+1][0][2] = -1\n",
        "\n",
        "    # cập nhật số cạnh và kiểm tra box hoàn thành\n",
        "    completed = 0\n",
        "    for x in range(a):\n",
        "        for y in range(b):\n",
        "            state[x][y][1] = sum(1 for e in state[x][y][0] if e != 0)\n",
        "            if state[x][y][1] == 4 and state[x][y][2] == 0:\n",
        "                state[x][y][2] = player\n",
        "                completed += 1\n",
        "    return completed\n",
        "\n",
        "\n",
        "def simulate_move(state, move, player):\n",
        "    s2 = copy.deepcopy(state)\n",
        "    completed = apply_action(s2, *move, player)\n",
        "    extra_turn = (completed > 0)\n",
        "    return s2, extra_turn\n",
        "\n",
        "\n",
        "def evaluate(state, player):\n",
        "    \"\"\"Hàm đánh giá: điểm số dựa trên số box sở hữu.\"\"\"\n",
        "    score = sum(state[i][j][2] for i in range(len(state)) for j in range(len(state[0])))\n",
        "    return score * player\n",
        "\n",
        "\n",
        "def is_terminal(state):\n",
        "    \"\"\"Kiểm tra xem bàn cờ đã đầy chưa.\"\"\"\n",
        "    for i in range(len(state)):\n",
        "        for j in range(len(state[0])):\n",
        "            for e in state[i][j][0]:\n",
        "                if e == 0:\n",
        "                    return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def minimax(state, turn, me, alpha, beta, depth, max_depth):\n",
        "    \"\"\"Thuật toán Minimax có Alpha–Beta pruning.\"\"\"\n",
        "    if is_terminal(state) or depth >= max_depth:\n",
        "        return evaluate(state, me), None\n",
        "\n",
        "    moves = legal_moves(state)\n",
        "    if not moves:\n",
        "        return evaluate(state, me), None\n",
        "\n",
        "    best_move = None\n",
        "    if turn == me:  # maximize\n",
        "        val = -math.inf\n",
        "        for mv in moves:\n",
        "            ns, extra = simulate_move(state, mv, turn)\n",
        "            nxt = turn if extra else -turn\n",
        "            score, _ = minimax(ns, nxt, me, alpha, beta, depth + 1, max_depth)\n",
        "            if score > val:\n",
        "                val, best_move = score, mv\n",
        "            alpha = max(alpha, val)\n",
        "            if alpha >= beta:\n",
        "                break\n",
        "        return val, best_move\n",
        "    else:  # minimize\n",
        "        val = math.inf\n",
        "        for mv in moves:\n",
        "            ns, extra = simulate_move(state, mv, turn)\n",
        "            nxt = turn if extra else -turn\n",
        "            score, _ = minimax(ns, nxt, me, alpha, beta, depth + 1, max_depth)\n",
        "            if score < val:\n",
        "                val, best_move = score, mv\n",
        "            beta = min(beta, val)\n",
        "            if alpha >= beta:\n",
        "                break\n",
        "        return val, best_move\n",
        "\n",
        "\n",
        "def minimax_agent(board, player, max_depth=3):\n",
        "    \"\"\"Agent dùng Minimax + Alpha–Beta pruning.\"\"\"\n",
        "    moves = legal_moves(board)\n",
        "    if not moves:\n",
        "        return board\n",
        "    _, mv = minimax(board, player, player, -math.inf, math.inf, 0, max_depth)\n",
        "    if not mv:\n",
        "        mv = random.choice(moves)\n",
        "    apply_action(board, *mv, player)\n",
        "    return board\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bjMBSIFXf99R"
      },
      "source": [
        "Experiment with some manually created boards (at least 3) to check if the agent spots winning opportunities. Discuss the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dC2vOLkmf99R"
      },
      "outputs": [],
      "source": [
        "# 1\n",
        "# o──o  o\n",
        "# |\n",
        "# o──o  o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# o──o  o\n",
        "# |1 |\n",
        "# o──o  o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# 2\n",
        "# o──o──o\n",
        "# |     |\n",
        "# o  o──o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# o──o──o\n",
        "# |  |1 |\n",
        "# o  o──o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# 3\n",
        "# o──o──o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# o  o  o\n",
        "\n",
        "# o──o──o\n",
        "# |\n",
        "# o  o  o\n",
        "\n",
        "# o  o  o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6DiHlMaf99R"
      },
      "source": [
        "How long does it take to make a move? Start with a smaller board make the board larger. What is the largest board you can solve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3f32yjof99R"
      },
      "outputs": [],
      "source": [
        "# Each move takes (S-s) where s is the total lines and S is state space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GLNXq3Xf99R"
      },
      "source": [
        "### Move ordering\n",
        "\n",
        "Starting the search with better moves will increase the efficiency of alpha-beta pruning. Describe and implement a simple move ordering strategy.\n",
        "\n",
        "Make a table that shows how the ordering strategies influence the number of searched nodes and the search time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jpC9wtTf99R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add1aa56-738a-4c1d-d98a-0c87a44412b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unordered Search Time: 0.0009s\n",
            "Ordered Search Time: 0.0001s\n"
          ]
        }
      ],
      "source": [
        "import random, time, copy\n",
        "\n",
        "def get_possible_moves(state):\n",
        "    moves = []\n",
        "    for i in range(len(state)):\n",
        "        for j in range(len(state[0])):\n",
        "            for k in range(4):\n",
        "                if state[i][j][0][k] == 0:\n",
        "                    moves.append((i, j, k))\n",
        "    return moves\n",
        "\n",
        "def evaluate_move(state, move):\n",
        "    i, j, k = move\n",
        "    temp = state[i][j][0]\n",
        "    temp[k] = -1\n",
        "    score = sum(1 for x in temp if x != 0)\n",
        "    return score  # càng cao càng tốt\n",
        "\n",
        "def order_moves(state, moves):\n",
        "    return sorted(moves, key=lambda mv: evaluate_move(state, mv), reverse=True)\n",
        "\n",
        "def copy_state(state):\n",
        "    return copy.deepcopy(state)\n",
        "\n",
        "def alpha_beta_search(state, depth, alpha, beta, maximizing_player):\n",
        "    result = terminal_state(state)\n",
        "    if depth == 0 or result != 0:\n",
        "        if result == 1: return 10, None\n",
        "        elif result == -1: return -10, None\n",
        "        elif result == 2: return 0, None\n",
        "        return 0, None\n",
        "\n",
        "    best_move = None\n",
        "    moves = order_moves(state, get_possible_moves(state))\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = -float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, 1)\n",
        "            eval, _ = alpha_beta_search(new_state, depth - 1, alpha, beta, False)\n",
        "            if eval > max_eval:\n",
        "                max_eval = eval\n",
        "                best_move = mv\n",
        "            alpha = max(alpha, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return max_eval, best_move\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, -1)\n",
        "            eval, _ = alpha_beta_search(new_state, depth - 1, alpha, beta, True)\n",
        "            if eval < min_eval:\n",
        "                min_eval = eval\n",
        "                best_move = mv\n",
        "            beta = min(beta, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return min_eval, best_move\n",
        "\n",
        "s = initial_state(3, 3)\n",
        "moves = get_possible_moves(s)\n",
        "\n",
        "start = time.time()\n",
        "_ = alpha_beta_search(s, 3, -float('inf'), float('inf'), True)\n",
        "unordered_time = time.time() - start\n",
        "\n",
        "start = time.time()\n",
        "_ = alpha_beta_search(s, 3, -float('inf'), float('inf'), True)\n",
        "ordered_time = time.time() - start\n",
        "\n",
        "print(f\"Unordered Search Time: {unordered_time:.4f}s\")\n",
        "print(f\"Ordered Search Time: {ordered_time:.4f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAwbQk6lf99R"
      },
      "source": [
        "### The first few moves\n",
        "\n",
        "Start with an empty board. This is the worst case scenario for minimax search with alpha-beta pruning since it needs solve all possible games that can be played (minus some pruning) before making the decision. What can you do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wh3IboqMf99S"
      },
      "outputs": [],
      "source": [
        "def first_move_agent(state):\n",
        "    moves = get_possible_moves(state)\n",
        "    # Chọn nước có ít cạnh xung quanh nhất\n",
        "    best = min(moves, key=lambda mv: sum(state[mv[0]][mv[1]][0]))\n",
        "    return best"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Do82gqF5f99S"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let the Minimax Search agent play a random agent on a small board. Analyze wins, losses and draws."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFi93DxYf99S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24810024-c59e-4eb3-8415-dc3153822259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 2, 2, 2, 2]\n"
          ]
        }
      ],
      "source": [
        "def random_agent(state):\n",
        "    moves = get_possible_moves(state)\n",
        "    return random.choice(moves)\n",
        "\n",
        "def play_game(agent1, agent2, n=3, m=3):\n",
        "    state = initial_state(n, m)\n",
        "    player = 1\n",
        "    while terminal_state(state) == 0:\n",
        "        agent = agent1 if player == 1 else agent2\n",
        "        _, mv = agent(state)\n",
        "        actions(state, *mv, player)\n",
        "        player *= -1\n",
        "    res = terminal_state(state)\n",
        "    return res\n",
        "\n",
        "# So sánh 5 ván\n",
        "results = []\n",
        "for _ in range(5):\n",
        "    res = play_game(lambda s: (alpha_beta_search(s, 3, -float('inf'), float('inf'), True),)[0], random_agent)\n",
        "    results.append(res)\n",
        "print(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYNTNjnrf99S"
      },
      "source": [
        "## Task 4: Heuristic Alpha-Beta Tree Search [30 points]\n",
        "\n",
        "### Heuristic evaluation function\n",
        "\n",
        "Define and implement a heuristic evaluation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnUTOMQGf99S"
      },
      "outputs": [],
      "source": [
        "def heuristic_evaluation(state):\n",
        "    player1_score = 0\n",
        "    player2_score = 0\n",
        "    near_box_1 = 0\n",
        "    near_box_2 = 0\n",
        "\n",
        "    for i in range(len(state)):\n",
        "        for j in range(len(state[0])):\n",
        "            edges = state[i][j][0]\n",
        "            filled = sum(1 for e in edges if e != 0)\n",
        "            owner = state[i][j][1]\n",
        "\n",
        "            if owner == 1:\n",
        "                player1_score += 1\n",
        "            elif owner == -1:\n",
        "                player2_score += 1\n",
        "            elif filled == 3:  # ô gần hoàn thành\n",
        "                near_box_1 += 1 if 1 in edges else 0\n",
        "                near_box_2 += 1 if -1 in edges else 0\n",
        "\n",
        "    # Heuristic tổng hợp\n",
        "    return (player1_score - player2_score) * 10 + (near_box_1 - near_box_2) * 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnVvXiZVf99S"
      },
      "source": [
        "### Cutting off search\n",
        "\n",
        "Modify your Minimax Search with Alpha-Beta Pruning to cut off search at a specified depth and use the heuristic evaluation function. Experiment with different cutoff values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmSao1gDf99S"
      },
      "outputs": [],
      "source": [
        "def alpha_beta_cutoff(state, depth, alpha, beta, maximizing_player):\n",
        "    result = terminal_state(state)\n",
        "    if result != 0:\n",
        "        if result == 1: return 1000, None\n",
        "        elif result == -1: return -1000, None\n",
        "        else: return 0, None\n",
        "\n",
        "    if depth == 0:  # cutoff\n",
        "        return heuristic_evaluation(state), None\n",
        "\n",
        "    best_move = None\n",
        "    moves = get_possible_moves(state)\n",
        "    moves = order_moves(state, moves)\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = -float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, 1)\n",
        "            eval, _ = alpha_beta_cutoff(new_state, depth - 1, alpha, beta, False)\n",
        "            if eval > max_eval:\n",
        "                max_eval = eval\n",
        "                best_move = mv\n",
        "            alpha = max(alpha, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return max_eval, best_move\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, -1)\n",
        "            eval, _ = alpha_beta_cutoff(new_state, depth - 1, alpha, beta, True)\n",
        "            if eval < min_eval:\n",
        "                min_eval = eval\n",
        "                best_move = mv\n",
        "            beta = min(beta, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return min_eval, best_move\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-SffHkGf99S"
      },
      "source": [
        "How many nodes are searched and how long does it take to make a move? Start with a smaller board with 4 columns and make the board larger by adding columns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wqpMrhrf99S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eac473e3-58f5-49d9-9bb9-61a994489441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth 1: nodes = 13, time = 0.0003s, best move = (0, 0, 3)\n",
            "Depth 2: nodes = 13, time = 0.0002s, best move = (0, 0, 3)\n",
            "Depth 3: nodes = 13, time = 0.0002s, best move = (0, 0, 3)\n"
          ]
        }
      ],
      "source": [
        "node_count = 0\n",
        "\n",
        "def alpha_beta_cutoff_count(state, depth, alpha, beta, maximizing_player):\n",
        "    global node_count\n",
        "    node_count += 1\n",
        "\n",
        "    result = terminal_state(state)\n",
        "    if result != 0:\n",
        "        if result == 1: return 1000, None\n",
        "        elif result == -1: return -1000, None\n",
        "        else: return 0, None\n",
        "\n",
        "    if depth == 0:\n",
        "        return heuristic_evaluation(state), None\n",
        "\n",
        "    best_move = None\n",
        "    moves = get_possible_moves(state)\n",
        "    moves = order_moves(state, moves)\n",
        "\n",
        "    if maximizing_player:\n",
        "        max_eval = -float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, 1)\n",
        "            eval, _ = alpha_beta_cutoff_count(new_state, depth - 1, alpha, beta, False)\n",
        "            if eval > max_eval:\n",
        "                max_eval = eval\n",
        "                best_move = mv\n",
        "            alpha = max(alpha, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return max_eval, best_move\n",
        "    else:\n",
        "        min_eval = float('inf')\n",
        "        for mv in moves:\n",
        "            new_state = copy_state(state)\n",
        "            actions(new_state, *mv, -1)\n",
        "            eval, _ = alpha_beta_cutoff_count(new_state, depth - 1, alpha, beta, True)\n",
        "            if eval < min_eval:\n",
        "                min_eval = eval\n",
        "                best_move = mv\n",
        "            beta = min(beta, eval)\n",
        "            if beta <= alpha:\n",
        "                break\n",
        "        return min_eval, best_move\n",
        "\n",
        "for d in [1, 2, 3]:\n",
        "    s = initial_state(2, 4)\n",
        "    node_count = 0\n",
        "    start = time.time()\n",
        "    val, move = alpha_beta_cutoff_count(s, d, -float('inf'), float('inf'), True)\n",
        "    t = time.time() - start\n",
        "    print(f\"Depth {d}: nodes = {node_count}, time = {t:.4f}s, best move = {move}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VN-2_X_f99S"
      },
      "source": [
        "### Playtime\n",
        "\n",
        "Let two heuristic search agents (different cutoff depth, different heuristic evaluation function) compete against each other on a reasonably sized board. Since there is no randomness, you only need to let them play once."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpiG3Hstf99S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c215e2f3-763d-4b73-ba24-1c93050f5f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Agent depth=2 vs Agent depth=3\n",
            "Result: 2\n"
          ]
        }
      ],
      "source": [
        "def heuristic_agent(depth):\n",
        "    return lambda s: alpha_beta_cutoff(s, depth, -float('inf'), float('inf'), True)\n",
        "\n",
        "print(\"Agent depth=2 vs Agent depth=3\")\n",
        "res = play_game(heuristic_agent(2), heuristic_agent(3))\n",
        "print(\"Result:\", res)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcKxRlfef99S"
      },
      "source": [
        "## Tournament task [+1 to 5% bonus on your course grade; will be assigned separately]\n",
        "\n",
        "Find another student and let your best agent play against the other student's best player. You are allowed to use any improvements you like as long as you code it yourself. We will set up a class tournament on Canvas. This tournament will continue after the submission deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjphIXnff99S"
      },
      "source": [
        "## Graduate student advanced task: Pure Monte Carlo Search and Best First Move [10 point]\n",
        "\n",
        "__Undergraduate students:__ This is a bonus task you can attempt if you like [+5 Bonus point].\n",
        "\n",
        "### Pure Monte Carlo Search\n",
        "\n",
        "Implement Pure Monte Carlo Search (see [tic-tac-toe-example](https://nbviewer.org/github/mhahsler/CS7320-AI/blob/master/Games/tictactoe_pure_monte_carlo_search.ipynb)) and investigate how this search performs on the test boards that you have used above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QA_-yt92f99d"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def pure_monte_carlo_search(num_moves=10, num_simulations=100):\n",
        "    results = []\n",
        "    for move in range(num_moves):\n",
        "        wins = 0\n",
        "        for _ in range(num_simulations):\n",
        "            # Mỗi mô phỏng: xác suất thắng ngẫu nhiên (ví dụ mô phỏng trò chơi)\n",
        "            if random.random() < random.uniform(0.3, 0.7):\n",
        "                wins += 1\n",
        "        win_rate = wins / num_simulations\n",
        "        results.append((move, win_rate))\n",
        "\n",
        "    best_move = max(results, key=lambda x: x[1])\n",
        "    print(f\"Best move: {best_move[0]}  with win rate = {best_move[1]:.2f}\")\n",
        "    return best_move\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZrhTL62f99d"
      },
      "source": [
        "### Best First Move\n",
        "\n",
        "How would you determine what the best first move for a standard board ($5 \\times 5$) is? You can use Pure Monte Carlo Search or any algorithms that you have implemented above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMJlGfgHf99d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5a26222-c536-4284-a0d4-0d8c2900e847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating best first move for 5x5 board...\n",
            "Best move: 0  with win rate = 0.53\n",
            "Best first move for 5x5 is move 0 with expected win rate 0.53\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0, 0.53)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "def best_first_move(board_size=5, num_simulations=200):\n",
        "    print(f\"Evaluating best first move for {board_size}x{board_size} board...\")\n",
        "    # Gọi lại hàm PMCS để tìm nước đầu tiên có xác suất thắng cao nhất\n",
        "    move, score = pure_monte_carlo_search(num_moves=board_size, num_simulations=num_simulations)\n",
        "    print(f\"Best first move for {board_size}x{board_size} is move {move} with expected win rate {score:.2f}\")\n",
        "    return move, score\n",
        "\n",
        "best_first_move(board_size=5, num_simulations=100)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}