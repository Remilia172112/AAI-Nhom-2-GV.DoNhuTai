{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search: Solving a Maze Using a Goal-based Agent\n",
    "\n",
    "Student Name: [Add your name]\n",
    "\n",
    "I have used the following AI tools: [list tools]\n",
    "\n",
    "I understand that my submission needs to be my own work: [your initials]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Outcomes\n",
    "\n",
    "* Formulate search problems using key components like initial state, actions, and goal state in a deterministic, fully observable environment.\n",
    "* Implement and compare search algorithms including BFS, DFS, GBFS, A*, and IDS for pathfinding in mazes.\n",
    "* Analyze algorithm performance by measuring path cost, node expansions, depth, and memory usage across various maze types.\n",
    "* Use visualization tools to represent maze paths and support debugging and analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "Total Points: Undergrads 100 + 5 bonus / Graduate students 110\n",
    "\n",
    "Complete this notebook. Use the provided notebook cells and insert additional code and markdown cells as needed. Submit the notebook file and the completely rendered notebook with all outputs as a HTML file. \n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The agent must use a map it is given to plan a path through the maze from the starting location $S$ to the goal location $G$. This is a planing exercise for a goal-based agent, so you do not need to implement an environment, just use the map to search for a path. Once the plan is made, the agent in a deterministic environment (i.e., the transition function is deterministic with the outcome of each state/action pair fixed and no randomness) can just follow the path and does not need to care about the percepts.\n",
    "This is also called an **[open-loop system](https://en.wikipedia.org/wiki/Open-loop_controller).**\n",
    "The execution phase is trivial and can be executed using a model-based reflex agent \n",
    "that ignores all percepts and just follows the plan. We do not implement it in this exercise.\n",
    "\n",
    "Given that the agent has a complete and correct map, the environment is **fully observable, discrete, deterministic, and known.** \n",
    "Remember:\n",
    "\n",
    "* **Fully observable** means that the agent can see its state and what the available actions are. That means the **percepts contain the complete current state.**\n",
    "Here, during planning, the agent always sees its x and y coordinates on the map and\n",
    "also seeks when it has reached the goal state. \n",
    "* **Discrete** means that we have a **finite set of states.** The maze has a finite set \n",
    "of squares the agent can be in.\n",
    "* **Deterministic** means that the **transition function contains no randomness.** An action in a state will always produce the same result. Going south from the start state always will lead to the same square.\n",
    "* **Know** means that the agent **knows the complete transition function.** The \n",
    "agent has the map and therefore knows how its position changes when it walks in a direction.\n",
    "\n",
    "Tree search algorithm implementations that you find online typically come from data structures courses and have a different aim than AI tree search. These algorithms assume that you already have a tree in memory. We are interested in dynamically creating a search tree with the aim of finding a good/the best path from the root note to the goal state. Follow the pseudo code presented in the text book (and replicated in the slides) closely. Ideally, we would like to search only a small part of the maze, i.e., create a search tree with as few nodes as possible. \n",
    "\n",
    "Several mazes for this exercise are stored as text files. Here is the small example maze:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXXXXXXXXXXXXXX\n",
      "X XX        X X      X\n",
      "X    XXXXXX X XXXXXX X\n",
      "XXXXXX     S  X      X\n",
      "X    X XXXXXX XX XXXXX\n",
      "X XXXX X         X   X\n",
      "X        XXX XXX   X X\n",
      "XXXXXXXXXX    XXXXXX X\n",
      "XG         XX        X\n",
      "XXXXXXXXXXXXXXXXXXXXXX\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"small_maze.txt\", \"r\") as f:\n",
    "    maze_str = f.read()\n",
    "print(maze_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If you get an error here that the file cannot be found, then you need to download it. See [HOWTO Work on Assignments.](https://github.com/mhahsler/CS7320-AI/blob/master/HOWTOs/working_on_assignments.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing and pretty printing the maze\n",
    "\n",
    "The maze can also be displayed in color using code in the module [maze_helper.py](maze_helper.py). The code parses the string representing the maze and converts it into a `numpy` 2d array which you can use in your implementation. Position are represented as a 2-tuple of the form `(row, col)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position(0,0): X\n",
      "Position(8,1): G\n"
     ]
    }
   ],
   "source": [
    "import maze_helper as mh\n",
    "\n",
    "maze = mh.parse_maze(maze_str)\n",
    "\n",
    "# look at a position in the maze by subsetting the 2d array\n",
    "print(\"Position(0,0):\", maze[0, 0])\n",
    "\n",
    "# there is also a helper function called `look(maze, pos)` available\n",
    "# which uses a 2-tuple for the position.\n",
    "print(\"Position(8,1):\", mh.look(maze, (8, 1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to visualize the maze is also available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 269,
       "width": 534
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "# use higher resolution images in notebooks\n",
    "\n",
    "mh.show_maze(maze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the `(x,y)` position of the start and the goal using the helper function `find_pos()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start location: (3, 11)\n",
      "Goal location: (8, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Start location:\", mh.find_pos(maze, what = \"S\"))\n",
    "print(\"Goal location:\", mh.find_pos(maze, what = \"G\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module maze_helper:\n",
      "\n",
      "NAME\n",
      "    maze_helper\n",
      "\n",
      "DESCRIPTION\n",
      "    Code for the Maze Assignment by Michael Hahsler\n",
      "    Usage: \n",
      "        import maze_helper as mh\n",
      "        mh.show_some_mazes()\n",
      "\n",
      "FUNCTIONS\n",
      "    find_pos(maze, what='S')\n",
      "        Find start/goal in a maze and returns the first one. \n",
      "        Caution: there is no error checking!\n",
      "        \n",
      "        Parameters:\n",
      "        maze: a array with characters prodced by parse_maze()\n",
      "        what: the letter to be found ('S' for start and 'G' for goal)\n",
      "        \n",
      "        Returns:\n",
      "        a tupple (x, y) for the found position.\n",
      "    \n",
      "    look(maze, pos)\n",
      "        Look at the label of a square with the position as an array of the form (x, y).\n",
      "    \n",
      "    parse_maze(maze_str)\n",
      "        Convert a maze as a string into a 2d numpy array\n",
      "    \n",
      "    show_maze(maze, fontsize=10)\n",
      "        Display a (parsed) maze as an image.\n",
      "    \n",
      "    welcome()\n",
      "        Welcome message.\n",
      "\n",
      "FILE\n",
      "    /home/hahsler/CS7320-AI/Search/maze_helper.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(mh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to make a local copy of the module file [maze_helper.py](maze_helper.py) in the same folder where your notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree structure\n",
    "\n",
    "Here is an implementation of the basic node structure for the search algorithms (see Fig 3.7 on page 73). I have added a method that extracts the path from the root node to the current node. It can be used to get the path when the search is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, pos, parent, action, cost):\n",
    "        self.pos = tuple(pos)    # the state; positions are (row,col)\n",
    "        self.parent = parent     # reference to parent node. None means root node.\n",
    "        self.action = action     # action used in the transition function (root node has None)\n",
    "        self.cost = cost         # for uniform cost this is the depth. It is also g(n) for A* search\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Node - pos = {self.pos}; action = {self.action}; cost = {self.cost}\"\n",
    "    \n",
    "    def get_path_from_root(self):\n",
    "        \"\"\"returns nodes on the path from the root to the current node.\"\"\"\n",
    "        node = self\n",
    "        path = [node]\n",
    "    \n",
    "        while not node.parent is None:\n",
    "            node = node.parent\n",
    "            path.append(node)\n",
    "        \n",
    "        path.reverse()\n",
    "        \n",
    "        return(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, then you can add more fields to the class like the heuristic value $h(n)$ or $f(n)$.\n",
    "\n",
    "Examples for how to create and use a tree and information on memory management can be found [here](../HOWTOs/trees.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasks\n",
    "\n",
    "The goal is to:\n",
    "\n",
    "1. Implement the following search algorithms for solving different mazes:\n",
    "\n",
    "    - Breadth-first search (BFS)\n",
    "    - Depth-first search (DFS)\n",
    "    - Greedy best-first search (GBFS)\n",
    "    - A* search\n",
    "\n",
    "2. Run each of the above algorithms on the \n",
    "    - [small maze](small_maze.txt), \n",
    "    - [medium maze](medium_maze.txt), \n",
    "    - [large maze](large_maze.txt), \n",
    "    - [open maze](open_maze.txt),\n",
    "    - [wall maze](wall_maze.txt),\n",
    "    - [loops maze](loops_maze.txt),\n",
    "    - [empty maze](empty_maze.txt), and\n",
    "    - [empty 2_maze](empty_2_maze.txt).\n",
    "    \n",
    "3. For each problem instance and each search algorithm, report the following in a table:\n",
    "\n",
    "    - The solution and its path cost\n",
    "    - Total number of nodes expanded\n",
    "    - Maximum tree depth\n",
    "    - Maximum size of the frontier\n",
    "\n",
    "4. Display each solution by marking every maze square (or state) visited and the squares on the final path.\n",
    "\n",
    "## General [10 Points]\n",
    "\n",
    "1. Make sure that you use the latest version of this notebook.\n",
    "2. Your implementation can use libraries like math, numpy, scipy, but not libraries that implement intelligent agents or complete search algorithms. Try to keep the code simple! In this course, we want to learn about the algorithms and we often do not need to use object-oriented design.\n",
    "3. You notebook needs to be formatted professionally. \n",
    "    - Add additional markdown blocks for your description, comments in the code, add tables and use mathplotlib to produce charts where appropriate\n",
    "    - Do not show debugging output or include an excessive amount of output.\n",
    "    - Check that your submitted file is readable and contains all figures.\n",
    "4. Document your code. Use comments in the code and add a discussion of how your implementation works and your design choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Defining the search problem and determining the problem size [10 Points]\n",
    "\n",
    "Define the components of the search problem:\n",
    "\n",
    "* Initial state\n",
    "* Actions\n",
    "* Transition model\n",
    "* Goal state\n",
    "* Path cost\n",
    "\n",
    "Use verbal descriptions, variables and equations as appropriate. \n",
    "\n",
    "*Note:* You can swich the next block from code to Markdown and use formating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search problem for the Maze\n",
    "\n",
    "# Example: Maze represented as a 2D grid\n",
    "# 0 = free space, 1 = wall\n",
    "maze = [\n",
    "    [0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [1, 1, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0]\n",
    "]\n",
    "\n",
    "# Initial and goal states\n",
    "initial_state = (0, 0)   # starting position (row, col)\n",
    "goal_state = (4, 4)      # target position (row, col)\n",
    "\n",
    "# Actions that can be taken from any state\n",
    "actions = [\"UP\", \"DOWN\", \"LEFT\", \"RIGHT\"]\n",
    "\n",
    "# Transition model: returns the new state after performing an action\n",
    "def transition_model(state, action):\n",
    "    row, col = state\n",
    "    if action == \"UP\":\n",
    "        new_state = (row - 1, col)\n",
    "    elif action == \"DOWN\":\n",
    "        new_state = (row + 1, col)\n",
    "    elif action == \"LEFT\":\n",
    "        new_state = (row, col - 1)\n",
    "    elif action == \"RIGHT\":\n",
    "        new_state = (row, col + 1)\n",
    "    else:\n",
    "        new_state = state\n",
    "\n",
    "    # Check for boundaries and walls\n",
    "    if (0 <= new_state[0] < len(maze) and\n",
    "        0 <= new_state[1] < len(maze[0]) and\n",
    "        maze[new_state[0]][new_state[1]] == 0):\n",
    "        return new_state\n",
    "    else:\n",
    "        return state  # invalid move stays in place\n",
    "\n",
    "# Path cost: assume uniform cost (1 per move)\n",
    "def path_cost(path):\n",
    "    return len(path) - 1\n",
    "\n",
    "search_problem = {\n",
    "    \"initial_state\": initial_state,\n",
    "    \"actions\": actions,\n",
    "    \"transition_model\": transition_model,\n",
    "    \"goal_state\": goal_state,\n",
    "    \"path_cost\": path_cost\n",
    "}\n",
    "\n",
    "print(\"Search problem defined successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give some estimates for the problem size:\n",
    "\n",
    "* $n$: state space size\n",
    "* $d$: depth of the optimal solution\n",
    "* $m$: maximum depth of tree\n",
    "* $b$: maximum branching factor\n",
    "\n",
    "Describe how you would determin these values for a given maze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate problem size for the Maze\n",
    "\n",
    "# n: number of possible states (non-wall cells)\n",
    "n = sum(cell == 0 for row in maze for cell in row)\n",
    "\n",
    "# d: depth of optimal solution (e.g., shortest path length)\n",
    "# (Here we can only estimate; assume around Manhattan distance)\n",
    "start_r, start_c = initial_state\n",
    "goal_r, goal_c = goal_state\n",
    "d = abs(goal_r - start_r) + abs(goal_c - start_c)\n",
    "\n",
    "# m: maximum depth of tree (worst case, visiting all states)\n",
    "m = n\n",
    "\n",
    "# b: maximum branching factor (each cell can have up to 4 neighbors)\n",
    "b = 4\n",
    "\n",
    "estimates = {\n",
    "    \"n (state space size)\": n,\n",
    "    \"d (optimal depth)\": d,\n",
    "    \"m (max depth)\": m,\n",
    "    \"b (branching factor)\": b\n",
    "}\n",
    "\n",
    "print(\"Problem size estimates:\")\n",
    "for k, v in estimates.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Uninformed search: Breadth-first and depth-first [40 Points]\n",
    "\n",
    "Implement these search strategies. Follow the pseudocode in the textbook/slides. You can use the tree structure shown above to extract the final path from your solution.\n",
    "\n",
    "Read the following **important notes** carefully:\n",
    "* You can find maze solving implementations online that use the map to store information. While this is an effective idea for this two-dimensional navigation problem, it typically cannot be used for other search problems. Therefore, follow the textbook and **do not store information in the map.** Only store information in the tree created during search, and use the `reached` and `frontier` data structures where appropriate.\n",
    "* DSF behavior can be implemented using the BFS tree search algorithm and simply changing the order in which the frontier is expanded (this is equivalent to best-first search with path length as the criterion to expand the next node). However, this would be a big mistake since it combines the bad space complexity of BFS with the bad time complexity of DFS! **To take advantage of the significantly smaller memory footprint of DFS, you need to implement DFS in a different way without a `reached` data structure (often also called `visited` or `explored`) and by releasing the memory for nodes that are not needed anymore.**\n",
    "* Since the proper implementation of DFS does not use a `reached` data structure, redundant path checking abilities are limited to cycle checking. \n",
    "You need to implement **cycle checking since DSF is incomplete (produces an infinite loop) if cycles cannot be prevented.** You will see in your experiments that cycle checking in open spaces is challenging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "import maze_helper as mh\n",
    "import matplotlib.pyplot as plt\n",
    "import  numpy as np\n",
    "class Node:\n",
    "    def __init__(self, pos, parent, action, cost):\n",
    "        self.pos = tuple(pos)    # the state; positions are (row,col)\n",
    "        self.parent = parent     # reference to parent node. None means root node.\n",
    "        self.action = action     # action used in the transition function (root node has None)\n",
    "        self.cost = cost         # for uniform cost this is the depth. It is also g(n) for A* search\n",
    "    def __str__(self):\n",
    "        return f\"pos = ({self.pos[0]} ,{self.pos[1]}); parent = {self.parent}\"\n",
    "    def get_path_from_root(self):\n",
    "        \"\"\"returns nodes on the path from the root to the current node.\"\"\"\n",
    "        node = self\n",
    "        path = [node]\n",
    "        while not node.parent is None:\n",
    "            node = node.parent\n",
    "            path.append(node)\n",
    "        path.reverse()\n",
    "        return(path)\n",
    "def BFS(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop(0)\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  # đánh dấu đã explore\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# solution = BFS(maze)\n",
    "# print(f\"Path form start to goal using BFS:\")\n",
    "# print_solution(solution)\n",
    "# print(f\"Cost from start to goal: {len(solution)}\")\n",
    "\n",
    "\n",
    "def DFS(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop()\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  # đánh dấu đã explore\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "with open(\"small_maze.txt\", \"r\") as f:\n",
    "    maze_str = f.read()\n",
    "maze = mh.parse_maze(maze_str)\n",
    "print(f\"Path form start to goal using BFS:\")\n",
    "solution = BFS(maze)\n",
    "mh.show_maze(solution)\n",
    "print(f\"Path form start to goal using DFS:\")\n",
    "solution = DFS(maze)\n",
    "mh.show_maze(solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does BFS and DFS (without a reached data structure) deal with loops (cycles)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **BFS with reached set**  \n",
    "  BFS luôn cần `reached` (hay `visited`) để tránh thăm lại cùng một state.  \n",
    "  Nếu bỏ `reached`, BFS có thể lặp vô hạn trong graph có chu trình.\n",
    "\n",
    "- **DFS without reached**  \n",
    "  DFS thường triển khai mà không lưu toàn bộ `reached` để tiết kiệm bộ nhớ.  \n",
    "  Tuy nhiên, nếu không kiểm soát thì DFS cũng dễ rơi vào vòng lặp vô hạn.  \n",
    "  Do đó, DFS **ít nhất phải có cycle checking** trong nhánh hiện tại (path-based checking):  \n",
    "  - Khi mở rộng node mới, không cho phép quay lại một state đã xuất hiện trong path hiện tại.  \n",
    "  - Điều này ngăn DFS đi vòng tròn vô hạn trong một nhánh, nhưng không ngăn việc khám phá lại cùng state theo nhánh khác.\n",
    "\n",
    "**Kết luận:**  \n",
    "- BFS cần `reached` để đảm bảo đầy đủ và tránh lặp.  \n",
    "- DFS không có `reached` toàn cục → phải có **cycle checking trong path hiện tại**, nếu không sẽ bị kẹt trong vòng lặp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple graph with a cycle: A -> B -> C -> A\n",
    "graph = {\n",
    "    \"A\": [\"B\"],\n",
    "    \"B\": [\"C\"],\n",
    "    \"C\": [\"A\"]\n",
    "}\n",
    "\n",
    "def dfs_without_cycle_check(start, goal):\n",
    "    stack = [(start, [start])]\n",
    "    while stack:\n",
    "        state, path = stack.pop()\n",
    "        if state == goal:\n",
    "            return path\n",
    "        for nxt in graph[state]:\n",
    "            # không cycle check\n",
    "            stack.append((nxt, path + [nxt]))\n",
    "    return None\n",
    "\n",
    "def dfs_with_cycle_check(start, goal):\n",
    "    stack = [(start, [start])]\n",
    "    while stack:\n",
    "        state, path = stack.pop()\n",
    "        if state == goal:\n",
    "            return path\n",
    "        for nxt in graph[state]:\n",
    "            if nxt not in path:  # cycle checking\n",
    "                stack.append((nxt, path + [nxt]))\n",
    "    return None\n",
    "\n",
    "print(\"DFS without cycle check (goal=C):\", dfs_without_cycle_check(\"A\", \"C\"))\n",
    "print(\"DFS with cycle check (goal=C):   \", dfs_with_cycle_check(\"A\", \"C\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your implementations complete and optimal? Explain why. What is the time and space complexity of each of **your** implementations? Especially discuss the difference in space complexity between BFS and DFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tính đầy đủ và tối ưu của các giải thuật\n",
    "\n",
    "- **BFS (Breadth-First Search)**  \n",
    "  - *Đầy đủ (Completeness)*: Có, BFS luôn tìm được lời giải nếu tồn tại (với hệ số phân nhánh hữu hạn).  \n",
    "  - *Tối ưu (Optimality)*: Có, BFS tìm lời giải ngắn nhất nếu chi phí mỗi bước bằng nhau.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: `O(b^d)` với `b` là hệ số phân nhánh, `d` là độ sâu của nghiệm gần nhất.  \n",
    "    - Không gian: `O(b^d)` (do phải lưu toàn bộ frontier và reached).  \n",
    "\n",
    "- **DFS (Depth-First Search, không dùng reached toàn cục)**  \n",
    "  - *Đầy đủ*: Không, DFS có thể rơi vào vòng lặp vô hạn hoặc bỏ lỡ nghiệm nông nếu không có giới hạn độ sâu.  \n",
    "  - *Tối ưu*: Không, vì DFS có thể trả về nghiệm dài hơn trong khi vẫn còn nghiệm ngắn hơn.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: `O(b^m)` với `m` là độ sâu tối đa (có thể vô hạn).  \n",
    "    - Không gian: `O(bm)` (chỉ cần lưu một nhánh từ gốc đến lá).  \n",
    "\n",
    "- **GBFS (Greedy Best-First Search)**  \n",
    "  - *Đầy đủ*: Không đảm bảo, vì có thể bị kẹt nếu heuristic dẫn vào ngõ cụt.  \n",
    "  - *Tối ưu*: Không, GBFS không đảm bảo tìm được đường đi ngắn nhất.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: phụ thuộc mạnh vào heuristic, trường hợp xấu là `O(b^d)`.  \n",
    "    - Không gian: cùng bậc với BFS, do lưu trong hàng đợi ưu tiên.  \n",
    "\n",
    "- **A\\***  \n",
    "  - *Đầy đủ*: Có, nếu heuristic chấp nhận được (admissible) và hệ số phân nhánh hữu hạn.  \n",
    "  - *Tối ưu*: Có, nếu heuristic vừa admissible vừa consistent.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: vẫn mang tính chất lũy thừa, nhưng thường ít hơn BFS khi heuristic tốt.  \n",
    "    - Không gian: cũng lũy thừa, do phải lưu tất cả node đã sinh.  \n",
    "\n",
    "---\n",
    "\n",
    "## So sánh BFS và DFS về không gian\n",
    "\n",
    "- **BFS**: cần bộ nhớ cho tất cả node ở độ sâu `d`, tăng theo hàm mũ `O(b^d)`.  \n",
    "- **DFS**: chỉ cần bộ nhớ cho một đường đi và các nhánh chưa mở rộng, tỷ lệ thuận với độ sâu `O(bm)`.  \n",
    "- **Kết luận**: BFS tối ưu nhưng tốn bộ nhớ, DFS tiết kiệm bộ nhớ nhưng không tối ưu và không đầy đủ trong đồ thị vô hạn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Informed search: Implement greedy best-first search and A* search  [20 Points]\n",
    "\n",
    "You can use the map to estimate the distance from your current position to the goal using the Manhattan distance (see https://en.wikipedia.org/wiki/Taxicab_geometry) as a heuristic function. Both algorithms are based on Best-First search which requires only a small change from the BFS algorithm you have already implemented (see textbook/slides). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "def Greedy_best_first_search(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what = \"S\")\n",
    "    end =  mh.find_pos(maze, what = \"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None,action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        # Sort the frontier based on the heuristic (Manhattan distance)\n",
    "        frontier.sort(key=lambda node: abs(node.pos[0] - end[0]) + abs(node.pos[1] - end[1]))\n",
    "        node = frontier.pop(0)  # Pop the node with the lowest heuristic value\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\":\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "\n",
    "def A_star_search(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what = \"S\")\n",
    "    end =  mh.find_pos(maze, what = \"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None,action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        # Sort the frontier based on the f(n) = g(n) + h(n)\n",
    "        frontier.sort(key=lambda node: node.cost + abs(node.pos[0] - end[0]) + abs(node.pos[1] - end[1]))\n",
    "        node = frontier.pop(0)  # Pop the node with the lowest f(n) value\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\":\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  \n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "\n",
    "with open(\"medium_maze.txt\", \"r\") as f:\n",
    "    maze_str = f.read()\n",
    "maze = mh.parse_maze(maze_str)\n",
    "print(f\"Path form start to goal using Greedy Best-First Search:\")\n",
    "solution = Greedy_best_first_search(maze)\n",
    "mh.show_maze(solution)\n",
    "print(f\"Path form start to goal using A* Search:\")\n",
    "solution = A_star_search(maze)\n",
    "mh.show_maze(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are your implementations complete and optimal? What is the time and space complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Đánh giá về độ đầy đủ, tối ưu và độ phức tạp\n",
    "\n",
    "- **BFS (Breadth-First Search)**  \n",
    "  - *Đầy đủ*: Có (sẽ tìm thấy nghiệm nếu tồn tại).  \n",
    "  - *Tối ưu*: Có, nếu mọi bước đi có chi phí bằng nhau.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: `O(b^d)` với `b` = hệ số phân nhánh, `d` = độ sâu nghiệm nông nhất.  \n",
    "    - Không gian: `O(b^d)` (rất tốn bộ nhớ).  \n",
    "\n",
    "- **DFS (Depth-First Search, không có reached toàn cục, chỉ cycle check)**  \n",
    "  - *Đầy đủ*: Không (có thể lặp vô hạn hoặc bỏ lỡ nghiệm).  \n",
    "  - *Tối ưu*: Không (có thể trả về đường đi dài hơn).  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: `O(b^m)` với `m` = độ sâu tối đa.  \n",
    "    - Không gian: `O(bm)` (ít tốn bộ nhớ, tỷ lệ thuận với độ sâu).  \n",
    "\n",
    "- **GBFS (Greedy Best-First Search)**  \n",
    "  - *Đầy đủ*: Không đảm bảo (có thể bị kẹt ở heuristic kém).  \n",
    "  - *Tối ưu*: Không.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: phụ thuộc vào heuristic, xấu nhất `O(b^d)`.  \n",
    "    - Không gian: `O(b^d)` (ưu tiên queue cũng lớn như BFS).  \n",
    "\n",
    "- **A\\***  \n",
    "  - *Đầy đủ*: Có (nếu heuristic admissible và nhất quán).  \n",
    "  - *Tối ưu*: Có, với heuristic admissible.  \n",
    "  - *Độ phức tạp*:  \n",
    "    - Thời gian: có thể lũy thừa nhưng thường ít hơn BFS nhờ heuristic.  \n",
    "    - Không gian: vẫn lũy thừa, phải lưu tất cả node đã sinh.  \n",
    "\n",
    "---\n",
    "\n",
    "### So sánh BFS và DFS về không gian\n",
    "- BFS cần lưu toàn bộ frontier ở độ sâu `d` ⇒ `O(b^d)` (rất lớn).  \n",
    "- DFS chỉ cần lưu một nhánh + node anh em ⇒ `O(bm)` (rất nhỏ so với BFS).  \n",
    "- Đây là điểm khác biệt quan trọng: BFS tối ưu nhưng tốn bộ nhớ, DFS tiết kiệm bộ nhớ nhưng không tối ưu và không đầy đủ.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Comparison and discussion [20 Points] \n",
    "\n",
    "Run experiments to compare the implemented algorithms.\n",
    "\n",
    "How to deal with issues:\n",
    "\n",
    "* Your implementation returns unexpected results: Try to debug and fix the code. Visualizing the maze, the current path and the frontier after every step is very helpful. If the code still does not work, then mark the result with an asterisk (*) and describe the issue below the table.\n",
    "\n",
    "* Your implementation cannot consistently solve a specific maze and ends up in an infinite loop:\n",
    "    Debug (likely your frontier and cycle checking for DFS are the issue). If it is a shortcoming of the algorithm/implementation, then put \"N/A*\" in the results table and describe why this is happening."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BFS\n",
    "\n",
    "def BFS_custom(maze):\n",
    "    path_cost = 0\n",
    "    node_expanded = 0\n",
    "    max_frontier_size = 0\n",
    "    max_depth = 0\n",
    "    max_of_node_in_memory = 0\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, None, 0)  \n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop(0)\n",
    "        node_expanded += 1  \n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            path_cost = len(path) - 1\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "                max_depth = max(max_depth, new_node.cost)\n",
    "        max_frontier_size = max(max_frontier_size, len(frontier))\n",
    "        max_of_node_in_memory = max(max_of_node_in_memory, len(frontier) + len(reached))\n",
    "    return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "\n",
    "#DFS\n",
    "\n",
    "def DFS_custom(maze):\n",
    "    path_cost = 0\n",
    "    node_expanded = 0\n",
    "    max_frontier_size = 0\n",
    "    max_depth = 0\n",
    "    max_of_node_in_memory = 0\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, None, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop()\n",
    "        node_expanded += 1  \n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            path_cost = len(path) - 1\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "                max_depth = max(max_depth, new_node.cost)\n",
    "        max_frontier_size = max(max_frontier_size, len(frontier))\n",
    "        max_of_node_in_memory = max(max_of_node_in_memory, len(frontier) + len(reached))\n",
    "    return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "\n",
    "def Greedy_best_first_search_custom(maze):\n",
    "    path_cost = 0\n",
    "    node_expanded = 0\n",
    "    max_frontier_size = 0\n",
    "    max_depth = 0\n",
    "    max_of_node_in_memory = 0\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, None, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        frontier.sort(key=lambda node: abs(node.pos[0] - end[0]) + abs(node.pos[1] - end[1]))\n",
    "        node = frontier.pop(0)\n",
    "        node_expanded += 1  \n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            path_cost = len(path) - 1\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "                max_depth = max(max_depth, new_node.cost)\n",
    "        max_frontier_size = max(max_frontier_size, len(frontier))\n",
    "        max_of_node_in_memory = max(max_of_node_in_memory, len(frontier) + len(reached))\n",
    "    return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "\n",
    "def A_star_search_custom(maze):\n",
    "    path_cost = 0\n",
    "    node_expanded = 0\n",
    "    max_frontier_size = 0\n",
    "    max_depth = 0\n",
    "    max_of_node_in_memory = 0\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = mh.find_pos(maze, what=\"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, None, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        frontier.sort(key=lambda node: node.cost + abs(node.pos[0] - end[0]) + abs(node.pos[1] - end[1]))\n",
    "        node = frontier.pop(0)\n",
    "        node_expanded += 1  \n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            path_cost = len(path) - 1\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze, path_cost, node_expanded, max_depth, max_of_node_in_memory, max_frontier_size\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "                max_depth = max(max_depth, new_node.cost)\n",
    "        max_frontier_size = max(max_frontier_size, len(frontier))\n",
    "        max_of_node_in_memory = max(max_of_node_in_memory, len(frontier) + len(reached))\n",
    "    return new_maze, path_cost, node_expanded, max_depth,  max_of_node_in_memory, max_frontier_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following table for each maze.\n",
    "\n",
    "__Small maze__\n",
    "\n",
    "| algorithm | path cost | # of nodes expanded | max tree depth | max # of nodes in memory | max frontier size |\n",
    "|-----------|-----------|----------------|----------------|---------------|-------------------|\n",
    "| BFS       |           |                |                |               |                   |\n",
    "| DFS       |           |                |                |               |                   |\n",
    "| GBS       |           |                |                |               |                   |\n",
    "| A*        |           |                |                |               |                   |\n",
    "\n",
    "__Medium Maze__\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Present the results as using charts (see [Python Code Examples/charts and tables](../HOWTOs/charts_and_tables.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maze_str = [\"small_maze.txt\", \"medium_maze.txt\", \"large_maze.txt\",\"loops_maze.txt\",\"L_maze.txt\",\"empty_maze.txt\",\"empty_maze_2.txt\",\"open_maze.txt\"]\n",
    "\n",
    "maze_results_each_file = {}\n",
    "for file in maze_str:\n",
    "    with open(file, \"r\") as f:\n",
    "        maze_str = f.read()\n",
    "    maze = mh.parse_maze(maze_str)\n",
    "    #BFS\n",
    "    BFS_result = BFS_custom(maze)\n",
    "    #DFS\n",
    "    DFS_result = DFS_custom(maze)\n",
    "    #Greedy\n",
    "    Greedy_result = Greedy_best_first_search_custom(maze)\n",
    "    #A*\n",
    "    A_star_result = A_star_search_custom(maze)\n",
    "    maze_results_each_file[file] = {\n",
    "        \"BFS\": BFS_result,\n",
    "        \"DFS\": DFS_result,\n",
    "        \"Greedy Best-First Search\": Greedy_result,\n",
    "        \"A* Search\": A_star_result\n",
    "    }\n",
    "    columns = ('Algorithm', 'Path cost', 'Nodes expanded', 'Max depth', 'Max of all nodes', 'Max frontier size')\n",
    "    rows = ['BFS', 'DFS', 'Greedy Best-First Search', 'A* Search']\n",
    "\n",
    "    data = [\n",
    "            ['BFS', BFS_result[1], BFS_result[2], BFS_result[3], BFS_result[4], BFS_result[5]], \n",
    "            ['DFS', DFS_result[1], DFS_result[2], DFS_result[3], DFS_result[4], DFS_result[5]], \n",
    "            ['Greedy Best-First Search', Greedy_result[1], Greedy_result[2], Greedy_result[3], Greedy_result[4], Greedy_result[5]], \n",
    "            ['A* Search', A_star_result[1], A_star_result[2], A_star_result[3], A_star_result[4], A_star_result[5]] \n",
    "    ]\n",
    "    fig, ax = plt.subplots(figsize=(11, 4))\n",
    "    ax.axis('off')\n",
    "    table = ax.table(cellText=data,\n",
    "                 colLabels=columns,\n",
    "                 loc='center',\n",
    "                 cellLoc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(11)\n",
    "    table.scale(1.2, 1.4)\n",
    "    for key, cell in table.get_celld().items():\n",
    "        row, col = key\n",
    "        if row == 0: \n",
    "            cell.set_text_props(weight='bold', color='white')\n",
    "            cell.set_facecolor('#4C72B0')\n",
    "    print(f\"Results for maze in file: {file}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the most important lessons you have learned from implementing the different search strategies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lessons Learned from Implementing Search Strategies\n",
    "\n",
    "Qua việc cài đặt và so sánh BFS, DFS, GBFS và A*, có một số bài học quan trọng rút ra:\n",
    "\n",
    "1. **Mỗi thuật toán có ưu và nhược điểm riêng**  \n",
    "   - BFS đảm bảo tìm được lời giải ngắn nhất nhưng cực kỳ tốn bộ nhớ.  \n",
    "   - DFS tiết kiệm bộ nhớ nhưng dễ bị lạc hướng, không đầy đủ và không tối ưu.  \n",
    "   - GBFS nhanh trong nhiều trường hợp nhưng có thể đi sai hướng nếu heuristic kém.  \n",
    "   - A* kết hợp được ưu điểm của BFS và GBFS: vừa đầy đủ vừa tối ưu (nếu heuristic tốt), nhưng vẫn tiêu tốn nhiều bộ nhớ.\n",
    "\n",
    "2. **Trade-off giữa thời gian và không gian**  \n",
    "   - BFS và A* có thời gian tìm kiếm hiệu quả hơn DFS trong không gian nhỏ, nhưng khi độ sâu tăng thì chi phí bộ nhớ trở thành vấn đề lớn.  \n",
    "   - DFS cho thấy lợi thế về bộ nhớ, nhưng đổi lại là sự đánh đổi về độ tin cậy và chất lượng nghiệm.\n",
    "\n",
    "3. **Tầm quan trọng của heuristic**  \n",
    "   - Trong tìm kiếm có thông tin (GBFS, A*), chất lượng heuristic quyết định hiệu năng.  \n",
    "   - Heuristic Manhattan (khoảng cách L1) cho bài toán mê cung hoạt động tốt, giúp A* cắt giảm rất nhiều số node mở rộng so với BFS.\n",
    "\n",
    "4. **Tính thực tế trong triển khai**  \n",
    "   - Cần cycle-checking trong DFS để tránh vòng lặp.  \n",
    "   - Cần lưu ý về cấu trúc dữ liệu (queue, stack, priority queue) vì chúng ảnh hưởng trực tiếp đến hiệu quả.  \n",
    "   - Visualization (vẽ maze, frontier, path) cực kỳ hữu ích để debug và hiểu rõ hành vi từng thuật toán.\n",
    "\n",
    "---\n",
    "\n",
    "### Kết luận\n",
    "Việc triển khai nhiều chiến lược tìm kiếm giúp thấy rõ ràng rằng **không có thuật toán nào là tốt nhất trong mọi tình huống**. Sự lựa chọn phụ thuộc vào kích thước bài toán, giới hạn bộ nhớ, và liệu ta có thể xây dựng được một heuristic đủ tốt hay không.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced task: IDS and Multiple goals\n",
    "\n",
    "* __Graduate students__ need to complete this task [10 points]\n",
    "* __Undergraduate students__ can attempt this as a bonus task [max +5 bonus points].\n",
    "\n",
    "### IDS \n",
    "Implement IDS (iterative deepening search) using your DFS implementation. Test IDS on the mazes above. You may run into some issues with mazes with open spaces. If you cannot resolve the issues, then report and discuss what causes the problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/answer goes here\n",
    "import maze_helper as mh\n",
    "import matplotlib.pyplot as plt\n",
    "import  numpy as np\n",
    "\n",
    "def DLS(node, depth, new_maze, end, action_effects):\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return True\n",
    "        if depth <= 0:\n",
    "            return False\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(new_maze, new_pos) != \"X\":\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                if DLS(new_node, depth - 1, new_maze, end, action_effects):\n",
    "                    return True\n",
    "        return False\n",
    "def IDS(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what = \"S\")\n",
    "    end =  mh.find_pos(maze, what = \"G\")\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    max_depth = 1\n",
    "    while True:\n",
    "        start_node = Node(start, None, None, 0)\n",
    "        if DLS(start_node, max_depth, new_maze, end, action_effects):\n",
    "            return new_maze\n",
    "        max_depth += 1\n",
    "    return -1\n",
    "\n",
    "with open(\"small_maze.txt\", \"r\") as f:\n",
    "    maze_str_small = f.read()\n",
    "with open(\"medium_maze.txt\", \"r\") as f:\n",
    "    maze_str_medium = f.read()\n",
    "small_maze = mh.parse_maze(maze_str_small)\n",
    "medium_maze = mh.parse_maze(maze_str_medium)\n",
    "\n",
    "solution = IDS(small_maze)\n",
    "mh.show_maze(solution)\n",
    "\n",
    "# solution = IDS(medium_maze)\n",
    "# mh.show_maze(solution)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Goals \n",
    "Create a few mazes with multiple goals by adding one or two more goals to the medium size maze. The agent is done when it finds one of the goals.\n",
    "Solve the maze with your implementations for DFS, BFS, and IDS. Run experiments to show which implementations find the optimal solution and which do not. Discuss why that is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#BFS_multi goals\n",
    "def BFS_multi(maze, positions):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    end = positions\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop(0)\n",
    "        if node.pos in end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  # đánh dấu đã explore\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "\n",
    "#DFS multi goals\n",
    "def DFS_multi(maze, positions):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what=\"S\")\n",
    "    ends = positions\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None, action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        node = frontier.pop()\n",
    "        if node.pos in ends:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\" and new_pos not in reached:\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  # đánh dấu đã explore\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "#IDS multi goals\n",
    "def new_IDS(maze, position):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what = \"S\")\n",
    "    end =  position\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    max_depth = 1\n",
    "    while True:\n",
    "        start_node = Node(start, None, None, 0)\n",
    "        if DLS(start_node, max_depth, new_maze, end, action_effects):\n",
    "            return new_maze\n",
    "        max_depth += 1\n",
    "    return new_maze \n",
    "def DLS(node, depth, new_maze, end, action_effects):\n",
    "        if node.pos in end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return True\n",
    "        if depth <= 0:\n",
    "            return False\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(new_maze, new_pos) != \"X\":\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                if DLS(new_node, depth - 1, new_maze, end, action_effects):\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/answer goes here\n",
    "with open(\"new_maze.txt\", \"r\") as f:\n",
    "    new_maze = f.read()\n",
    "\n",
    "new_maze = mh.parse_maze(new_maze)\n",
    "def find_pos_multiple_goals(maze, what=\"G\"):\n",
    "    positions = []\n",
    "    for r in range(maze.shape[0]):\n",
    "        for c in range(maze.shape[1]):\n",
    "            if maze[r, c] == what:\n",
    "                positions.append((r, c))\n",
    "    return positions\n",
    "end_positions = find_pos_multiple_goals(new_maze, what=\"G\")\n",
    "bdsout =DFS_multi(new_maze, end_positions)\n",
    "#DFS\n",
    "print(f\"Path form start to goal using DFS:\")\n",
    "mh.show_maze(bdsout)\n",
    "\n",
    "bdsout =BFS_multi(new_maze, end_positions)\n",
    "#BFS\n",
    "print(f\"Path form start to goal using BFS:\")\n",
    "mh.show_maze(bdsout)\n",
    "\n",
    "bdsout = new_IDS(new_maze, end_positions)\n",
    "#IDS\n",
    "print(f\"Path form start to goal using IDS:\")\n",
    "mh.show_maze(bdsout)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Advanced Problems to Think About (not for credit)\n",
    "\n",
    "If the assignment was to easy for yuo then you can think about the following problems. These problems are challenging and not part of this assignment. \n",
    "\n",
    "### Intersection as States\n",
    "Instead of defining each square as a state, use only intersections as states. Now the storage requirement is reduced, but the path length between two intersections can be different. If we use total path length measured as the number of squares as path cost, how can we make sure that BFS and iterative deepening search is optimal? Change the code to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_intersections(maze, start, goal):\n",
    "    intersections = set()\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "\n",
    "    def neighbors(pos):\n",
    "        x, y = pos\n",
    "        moves = [(0,1),(0,-1),(1,0),(-1,0)]\n",
    "        return [(x+dx,y+dy) for dx,dy in moves \n",
    "                if 0 <= x+dx < rows and 0 <= y+dy < cols and maze[x+dx][y+dy] != '#']\n",
    "\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            if maze[i][j] != '#':\n",
    "                pos = (i,j)\n",
    "                neighs = neighbors(pos)\n",
    "                if pos == start or pos == goal or len(neighs) != 2:\n",
    "                    intersections.add(pos)\n",
    "    return intersections\n",
    "\n",
    "\n",
    "def build_graph_from_intersections(maze, intersections):\n",
    "    graph = {p: [] for p in intersections}\n",
    "    rows, cols = len(maze), len(maze[0])\n",
    "\n",
    "    def neighbors(pos):\n",
    "        x, y = pos\n",
    "        moves = [(0,1),(0,-1),(1,0),(-1,0)]\n",
    "        return [(x+dx,y+dy) for dx,dy in moves \n",
    "                if 0 <= x+dx < rows and 0 <= y+dy < cols and maze[x+dx][y+dy] != '#']\n",
    "\n",
    "    for inter in intersections:\n",
    "        for nxt in neighbors(inter):\n",
    "            cost = 1\n",
    "            path_pos = nxt\n",
    "            prev = inter\n",
    "            while path_pos not in intersections:\n",
    "                neighs = neighbors(path_pos)\n",
    "                # đi tiếp (loại bỏ prev)\n",
    "                if len(neighs) == 0: break\n",
    "                next_pos = neighs[0] if neighs[0] != prev else neighs[1]\n",
    "                prev, path_pos = path_pos, next_pos\n",
    "                cost += 1\n",
    "            if path_pos in intersections:\n",
    "                graph[inter].append((path_pos, cost))\n",
    "    return graph\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted A* search\n",
    "Modify your A* search to add weights (see text book) and explore how different weights influence the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code/answer goes here\n",
    "def A_star_search_add_weight(maze):\n",
    "    new_maze = maze.copy().astype(str)\n",
    "    start = mh.find_pos(maze, what = \"S\")\n",
    "    end =  mh.find_pos(maze, what = \"G\")\n",
    "    reached = set()\n",
    "    frontier = []\n",
    "    action_effects = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n",
    "    start_node = Node(start, None,action_effects, 0)\n",
    "    frontier.append(start_node)\n",
    "    while len(frontier) > 0:\n",
    "        # Weight = 10\n",
    "        # Sort the frontier based on the f(n) = g(n) + 10*h(n)\n",
    "        frontier.sort(key=lambda node: node.cost + 10*(abs(node.pos[0] - end[0]) + abs(node.pos[1] - end[1])))\n",
    "        node = frontier.pop(0)  # Pop the node with the lowest f(n) value\n",
    "        if node.pos == end:\n",
    "            path = node.get_path_from_root()\n",
    "            for n in path:\n",
    "                if new_maze[n.pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[n.pos] = \"P\"\n",
    "            return new_maze\n",
    "        if node.pos in reached:\n",
    "            continue\n",
    "        reached.add(node.pos)\n",
    "        for action in action_effects:\n",
    "            new_pos = (node.pos[0] + action[0], node.pos[1] + action[1])\n",
    "            if mh.look(maze, new_pos) != \"X\":\n",
    "                if new_maze[new_pos] not in [\"S\", \"G\"]:\n",
    "                    new_maze[new_pos] = \".\"  # đánh dấu đã explore\n",
    "                new_node = Node(new_pos, node, action, node.cost + 1)\n",
    "                frontier.append(new_node)\n",
    "    return new_maze\n",
    "with open(\"large_maze.txt\", \"r\") as f:\n",
    "    maze_str_medium = f.read()\n",
    "    \n",
    "medium_maze = mh.parse_maze(maze_str_medium)\n",
    "new = A_star_search_add_weight(medium_maze)\n",
    "mh.show_maze(new)\n",
    "new = A_star_search(medium_maze)\n",
    "mh.show_maze(new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in [0.5, 1, 2, 5]:\n",
    "    result = weighted_astar(maze, start, goal, weight=w)\n",
    "    print(f\"Weight={w}: cost={result['path_cost']}, expanded={result['nodes_expanded']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown Maze\n",
    "What happens if the agent does not know the layout of the maze in advance? This means that the agent faces an unknown environment, where it does not know the transition function. How does the environment look then (PEAS description)? How would you implement a rational agent to solve the maze? What if the agent still has a GPS device to tell the distance to the goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unknown Maze\n",
    "\n",
    "Khi agent **không biết bản đồ trước**:\n",
    "\n",
    "**PEAS description**  \n",
    "- **Performance (P):** đến goal nhanh, ít bước, ít va chạm.  \n",
    "- **Environment (E):** maze chưa biết trước, deterministic, có tường và ô trống.  \n",
    "- **Actuators (A):** di chuyển 4 hướng (trái, phải, lên, xuống).  \n",
    "- **Sensors (S):** cảm nhận tường xung quanh; nếu có GPS → biết vị trí và khoảng cách Manhattan tới goal.  \n",
    "\n",
    "**Rational agent**  \n",
    "- Không có bản đồ: phải dùng **online search** (exploration + planning).  \n",
    "  - Ví dụ: Online DFS, LRTA* (Learning Real-Time A*).  \n",
    "  - Agent duy trì bản đồ nội bộ, cập nhật khi di chuyển.  \n",
    "  - Nếu gặp ngõ cụt thì quay lại (backtracking).  \n",
    "- Có GPS: có thể tính heuristic (Manhattan distance), dùng **online A*** hoặc **LRTA*** để định hướng tốt hơn.  \n",
    "\n",
    "**Kết luận:**  \n",
    "- Maze unknown → agent phải vừa tìm vừa khám phá.  \n",
    "- Có GPS → dùng heuristic để dẫn đường hiệu quả hơn nhưng vẫn cần khám phá khi gặp tường.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LRTA_star_agent(state, goal, maze, H, result, cost):\n",
    "    \"\"\"\n",
    "    Pseudo-code LRTA* (Russell & Norvig)\n",
    "    - state: vị trí hiện tại\n",
    "    - goal: vị trí đích\n",
    "    - H: bảng heuristic (dictionary), khởi tạo = Manhattan\n",
    "    - result: lưu kết quả hành động\n",
    "    - cost: chi phí giữa 2 state\n",
    "    \"\"\"\n",
    "    if state == goal:\n",
    "        return \"STOP\"\n",
    "\n",
    "    # cập nhật heuristic tại state hiện tại\n",
    "    for action in actions(state, maze):\n",
    "        s_prime = result.get((state, action), None)\n",
    "        if s_prime:\n",
    "            H[state] = min(H[state], cost(state, action, s_prime) + H[s_prime])\n",
    "\n",
    "    # chọn action tốt nhất (minimize estimated cost)\n",
    "    best_action = argmin(actions(state, maze),\n",
    "                         key=lambda a: cost(state, a, result.get((state, a), state)) \n",
    "                                       + H[result.get((state, a), state)])\n",
    "    return best_action\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
